{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘\tNNG,*,T,오늘,*,*,*,*\n",
      "은\tJX,*,T,은,*,*,*,*\n",
      "좋\tVA,*,T,좋,*,*,*,*\n",
      "은\tETM,*,T,은,*,*,*,*\n",
      "날\tNNG,*,T,날,*,*,*,*\n",
      ",\tSC,*,*,*,*,*,*,*\n",
      "행복\tNNG,정적사태,T,행복,*,*,*,*\n",
      "한\tXSA+ETM,*,T,한,Inflect,XSA,ETM,하/XSA/*+ᆫ/ETM/*\n",
      "삶\tNNG,행위,T,삶,*,*,*,*\n",
      "을\tJKO,*,T,을,*,*,*,*\n",
      "누리\tVV,*,F,누리,*,*,*,*\n",
      "자\tEF,*,F,자,*,*,*,*\n",
      ".\tSF,*,*,*,*,*,*,*\n",
      "EOS\n",
      "\n",
      "['오늘\\tNNG,*,T,오늘,*,*,*,*', '은\\tJX,*,T,은,*,*,*,*', '좋\\tVA,*,T,좋,*,*,*,*', '은\\tETM,*,T,은,*,*,*,*', '날\\tNNG,*,T,날,*,*,*,*', ',\\tSC,*,*,*,*,*,*,*', '11\\tSN,*,*,*,*,*,*,*', '행복\\tNNG,정적사태,T,행복,*,*,*,*', '한\\tXSA+ETM,*,T,한,Inflect,XSA,ETM,하/XSA/*+ᆫ/ETM/*', '삶\\tNNG,행위,T,삶,*,*,*,*', '을\\tJKO,*,T,을,*,*,*,*', '누리\\tVV,*,F,누리,*,*,*,*', '자\\tEF,*,F,자,*,*,*,*', '.\\tSF,*,*,*,*,*,*,*', 'EOS', '']\n",
      "['오늘', '은', '좋', '은', '날', ',', '11', '행복', '한', '삶', '을', '누리', '자', '.']\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "m = MeCab.Tagger()\n",
    "a = m.parse(\"오늘은 좋은날, 행복한 삶을 누리자.\")\n",
    "print(a)\n",
    "print(m.parse(\"오늘은 좋은날, 11행복한 삶을 누리자.\").split('\\n'))\n",
    "a = list(map(lambda morphs : morphs.split('\\t')[0], m.parse(\"오늘은 좋은날, 11행복한 삶을 누리자.\").split('\\n')))[:-2]\n",
    "# a = list(map(lambda morphs : morphs[0], m.parse(\"오늘은 좋은날, 11행복한 삶을 누리자.\").split('\\n')))\n",
    "print(a)\n",
    "# print(m.('화학 이외의 것'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mecab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmecab\u001b[39;00m\n\u001b[0;32m      2\u001b[0m mecab \u001b[39m=\u001b[39m mecab\u001b[39m.\u001b[39mMeCab()\n\u001b[0;32m      4\u001b[0m mecab\u001b[39m.\u001b[39mmorphs(\u001b[39m'\u001b[39m\u001b[39m영등포구청역에 있는 맛집 좀 알려주세요.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mecab'"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "mecab = MeCab.()\n",
    "\n",
    "mecab.morphs('영등포구청역에 있는 맛집 좀 알려주세요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../raw_data/../bert_data/ ../raw_data/../json_data/\n",
      "../raw_data/../bert_data/ train\\korean.train.1.0.json\n",
      "../raw_data/../json_data/train\\korean.train.1.0.json {'dataset': ['train'], 'raw_path': '../raw_data/../json_data/', 'save_path': '../raw_data/../bert_data/', 'n_cpus': 16, 'oracle_mode': 'greedy', 'min_src_ntokens': 5, 'max_src_ntokens': 200, 'min_nsents': 3, 'max_nsents': 100} ../raw_data/../bert_data/train\\korean.train.1.0.bert.pt\n",
      "../raw_data/../bert_data/ ../raw_data/../json_data/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import MeCab\n",
    "import easydict\n",
    "from prepro.data_builder import format_to_bert\n",
    "def run(path):\n",
    "    for path2 in ['train','valid']:\n",
    "        DATAPATH = path+path2\n",
    "        filenames = [x for x in os.listdir (DATAPATH) if x.endswith('json')]\n",
    "        list_dic = []\n",
    "\n",
    "        for file in filenames:\n",
    "            filelocation = os.path.join(DATAPATH, file)\n",
    "\n",
    "            with open(filelocation, 'r',encoding='utf-8') as json_file:\n",
    "                data = json.load(json_file)['documents']\n",
    "\n",
    "                for x in tqdm (range(len(data))):\n",
    "                    text = data[x]['text']\n",
    "                    text = str(text).replace('\"', \"'\")\n",
    "\n",
    "                    extractive = data[x]['extractive']\n",
    "                    for index, value in enumerate(extractive):\n",
    "                        if value == None:\n",
    "                            extractive[index] = 0\n",
    "\n",
    "                    p = re.compile('(?<=sentence\\'\\: \\')(.*?)(?=\\'highlight_indices)')\n",
    "                    texts = p.findall(text)\n",
    "\n",
    "                    sentences = []\n",
    "                    for t in texts:\n",
    "                        sentence = t[:-3]\n",
    "                        sentences.append(sentence)\n",
    "\n",
    "                    mydict = {}\n",
    "                    mydict['text'] = sentences\n",
    "                    mydict['extractive'] = extractive\n",
    "                    list_dic.append(mydict)\n",
    "        \n",
    "        with open(f\"{path+path2}.json\", 'w') as fh:\n",
    "            json.dump(list_dic, fh)\n",
    "\n",
    "        def list_chunk(lst, n):\n",
    "            return [lst[i:i+n] for i in range(0, len(lst), n)]\n",
    "        if path2 == 'train':\n",
    "            with open(f\"{path+path2}.json\", 'r') as fh:\n",
    "                data = json.load(fh)\n",
    "            data_chunked = list_chunk(data, 32507) ## 전체 데이터를 10개로 분할\n",
    "            for i, d in enumerate(data_chunked):\n",
    "                with open(f\"{path+path2}.{i}.json\".format(i), 'w') as fh:\n",
    "                    json.dump(d, fh)\n",
    "        mecab = MeCab.Tagger()\n",
    "        DATAPATH = path\n",
    "        filenames = [x for x in os.listdir (DATAPATH) if path2 in x and x.endswith('json')]\n",
    "        print(filenames)\n",
    "        trainfiles = []\n",
    "        for f in filenames[1:-1]:\n",
    "            trainfiles.append(f[:-5])\n",
    "        print(trainfiles)\n",
    "        for set_name in trainfiles:\n",
    "            print(\"Processing ... \", set_name)\n",
    "\n",
    "            with open(\"{}/{}.json\".format(path,set_name), 'r',encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "                list_dic = []\n",
    "                for x in tqdm(range(len(data))):\n",
    "                    text = data[x]['text']\n",
    "                    extractive = data[x]['extractive']\n",
    "\n",
    "                    sentences = []\n",
    "                    for sentence in text:\n",
    "                        sentence_morph = ' '.join(list(map(lambda morphs : morphs.split('\\t')[0], m.parse(sentence).split('\\n')))[:-2])\n",
    "                        sentences.append(sentence_morph)\n",
    "\n",
    "                    extractives = []\n",
    "                    for e in extractive:\n",
    "                        extractives.append(sentences[e])\n",
    "\n",
    "                    src = [i.split(' ') for i in sentences]\n",
    "                    tgt = [i.split(' ') for i in extractives]\n",
    "\n",
    "                    mydict = {}\n",
    "                    mydict['src'] = src\n",
    "                    mydict['tgt'] = tgt\n",
    "                    list_dic.append(mydict)\n",
    "\n",
    "                jsonfilelocation = path+'../json_data/' + path2\n",
    "                os.makedirs(jsonfilelocation, exist_ok=True)\n",
    "\n",
    "                temp = []\n",
    "                DATA_PER_FILE = 50\n",
    "\n",
    "                for i,a in enumerate(tqdm(list_dic)):\n",
    "                    if (i+1)%DATA_PER_FILE!=0:\n",
    "                        temp.append(a)\n",
    "                    else:\n",
    "                        temp.append(a)\n",
    "                        filename = 'korean.'+ set_name + '.' + str(i//DATA_PER_FILE)+'.json'\n",
    "                        with open(os.path.join(jsonfilelocation, filename), \"w\", encoding='utf-8') as json_file:\n",
    "                            json.dump(temp, json_file, ensure_ascii=False)\n",
    "                        temp = []\n",
    "\n",
    "                    #마지막에 남은 데이터 있으면 추가로 append\n",
    "                    if len(temp) != 0:\n",
    "                        filename = 'korean.'+ set_name + '.' + str(i//DATA_PER_FILE + 1)+'.json'\n",
    "                        with open(os.path.join(jsonfilelocation, filename), \"w\", encoding='utf-8') as json_file:\n",
    "                            json.dump(temp, json_file, ensure_ascii=False)\n",
    "        set_name = path2\n",
    "\n",
    "        bertfilelocation = path+'../bert_data/'\n",
    "        os.makedirs(bertfilelocation, exist_ok=True)\n",
    "        print(bertfilelocation, path+'../json_data/')\n",
    "        args = easydict.EasyDict({\n",
    "        \"dataset\": [set_name], \n",
    "        \"raw_path\": path+'../json_data/',\n",
    "        \"save_path\": bertfilelocation,\n",
    "        \"n_cpus\":16,\n",
    "        \"oracle_mode\": \"greedy\",\n",
    "        \"min_src_ntokens\": 5,\n",
    "        \"max_src_ntokens\": 200,\n",
    "        \"min_nsents\": 3,\n",
    "        \"max_nsents\": 100,\n",
    "        })\n",
    "        format_to_bert(args)\n",
    "if __name__ == '__main__':\n",
    "    run('../raw_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyrouge\n",
      "  Downloading pyrouge-0.1.3.tar.gz (60 kB)\n",
      "     ---------------------------------------- 60.5/60.5 kB 3.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: pyrouge\n",
      "  Building wheel for pyrouge (setup.py): started\n",
      "  Building wheel for pyrouge (setup.py): finished with status 'done'\n",
      "  Created wheel for pyrouge: filename=pyrouge-0.1.3-py3-none-any.whl size=191648 sha256=4a2ef18524525589250c7a8d1e565a13156a5fabcf1cf0fd9c8fdc481dbc2d8f\n",
      "  Stored in directory: c:\\users\\answl\\appdata\\local\\pip\\cache\\wheels\\00\\85\\fd\\ccd28e53c9f6a691e6ea96050a0cad95f9a4a6361269d765ca\n",
      "Successfully built pyrouge\n",
      "Installing collected packages: pyrouge\n",
      "Successfully installed pyrouge-0.1.3\n",
      "Collecting https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
      "  Downloading https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
      "     | 202.3 kB 336.9 kB/s 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pyrouge in c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages (0.1.3)\n",
      "Name: pyrouge\n",
      "Version: 0.1.3\n",
      "Summary: A Python wrapper for the ROUGE summarization evaluation package.\n",
      "Home-page: https://github.com/noutenki/pyrouge\n",
      "Author: Benjamin Heinzerling, Anders Johannsen\n",
      "Author-email: benjamin.heinzerling@h-its.org\n",
      "License: LICENSE.txt\n",
      "Location: c:\\users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pyrouge'...\n"
     ]
    }
   ],
   "source": [
    "!pip install pyrouge --upgrade\n",
    "\n",
    "!pip install https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
    "\n",
    "!pip install pyrouge\n",
    "\n",
    "!pip show pyrouge\n",
    "\n",
    "!git clone https://github.com/andersjo/pyrouge.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21564\\677415331.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'"
     ]
    }
   ],
   "source": [
    "from pyrouge import Rouge155\n",
    "\n",
    "!pyrouge_set_rouge_path '/content/pyrouge/tools/ROUGE-1.5.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-11-05 13:04:59,763 INFO] Device ID -1\n",
      "[2022-11-05 13:04:59,763 INFO] Device cpu\n",
      "\n",
      "Downloading:   0%|          | 0.00/425 [00:00<?, ?B/s]\n",
      "Downloading: 100%|██████████| 425/425 [00:00<00:00, 425kB/s]\n",
      "c:\\Users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\answl\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "\n",
      "Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]\n",
      "Downloading:   1%|          | 3.12M/445M [00:00<00:14, 31.1MB/s]\n",
      "Downloading:   2%|▏         | 6.94M/445M [00:00<00:12, 35.1MB/s]\n",
      "Downloading:   2%|▏         | 10.7M/445M [00:00<00:11, 36.2MB/s]\n",
      "Downloading:   3%|▎         | 14.4M/445M [00:00<00:11, 36.5MB/s]\n",
      "Downloading:   4%|▍         | 18.2M/445M [00:00<00:11, 37.0MB/s]\n",
      "Downloading:   5%|▍         | 22.0M/445M [00:00<00:11, 37.3MB/s]\n",
      "Downloading:   6%|▌         | 25.8M/445M [00:00<00:11, 37.5MB/s]\n",
      "Downloading:   7%|▋         | 29.6M/445M [00:00<00:11, 37.6MB/s]\n",
      "Downloading:   8%|▊         | 33.4M/445M [00:00<00:10, 37.7MB/s]\n",
      "Downloading:   8%|▊         | 37.2M/445M [00:01<00:10, 37.7MB/s]\n",
      "Downloading:   9%|▉         | 41.8M/445M [00:01<00:10, 40.2MB/s]\n",
      "Downloading:  11%|█         | 48.0M/445M [00:01<00:08, 46.7MB/s]\n",
      "Downloading:  12%|█▏        | 53.6M/445M [00:01<00:07, 49.4MB/s]\n",
      "Downloading:  13%|█▎        | 58.5M/445M [00:01<00:08, 47.6MB/s]\n",
      "Downloading:  15%|█▍        | 66.1M/445M [00:01<00:06, 55.5MB/s]\n",
      "Downloading:  16%|█▌        | 71.7M/445M [00:01<00:06, 54.5MB/s]\n",
      "Downloading:  18%|█▊        | 78.2M/445M [00:01<00:06, 57.6MB/s]\n",
      "Downloading:  19%|█▉        | 84.1M/445M [00:01<00:06, 57.8MB/s]\n",
      "Downloading:  20%|██        | 90.2M/445M [00:01<00:06, 58.5MB/s]\n",
      "Downloading:  22%|██▏       | 96.2M/445M [00:02<00:05, 59.0MB/s]\n",
      "Downloading:  23%|██▎       | 102M/445M [00:02<00:05, 59.2MB/s] \n",
      "Downloading:  24%|██▍       | 109M/445M [00:02<00:05, 59.8MB/s]\n",
      "Downloading:  26%|██▌       | 115M/445M [00:02<00:05, 59.2MB/s]\n",
      "Downloading:  27%|██▋       | 121M/445M [00:02<00:05, 60.4MB/s]\n",
      "Downloading:  29%|██▊       | 127M/445M [00:02<00:05, 60.2MB/s]\n",
      "Downloading:  30%|██▉       | 133M/445M [00:02<00:05, 59.6MB/s]\n",
      "Downloading:  31%|███▏      | 139M/445M [00:02<00:05, 56.5MB/s]\n",
      "Downloading:  33%|███▎      | 147M/445M [00:02<00:04, 60.0MB/s]\n",
      "Downloading:  34%|███▍      | 153M/445M [00:02<00:04, 60.1MB/s]\n",
      "Downloading:  36%|███▌      | 159M/445M [00:03<00:04, 60.2MB/s]\n",
      "Downloading:  37%|███▋      | 165M/445M [00:03<00:04, 59.5MB/s]\n",
      "Downloading:  38%|███▊      | 171M/445M [00:03<00:04, 59.9MB/s]\n",
      "Downloading:  40%|███▉      | 177M/445M [00:03<00:04, 61.3MB/s]\n",
      "Downloading:  41%|████▏     | 184M/445M [00:03<00:04, 59.4MB/s]\n",
      "Downloading:  43%|████▎     | 190M/445M [00:03<00:04, 58.8MB/s]\n",
      "Downloading:  44%|████▍     | 196M/445M [00:03<00:04, 59.1MB/s]\n",
      "Downloading:  45%|████▌     | 202M/445M [00:03<00:04, 59.4MB/s]\n",
      "Downloading:  47%|████▋     | 208M/445M [00:03<00:03, 60.6MB/s]\n",
      "Downloading:  48%|████▊     | 214M/445M [00:04<00:04, 57.5MB/s]\n",
      "Downloading:  50%|████▉     | 221M/445M [00:04<00:03, 61.2MB/s]\n",
      "Downloading:  51%|█████     | 227M/445M [00:04<00:03, 60.4MB/s]\n",
      "Downloading:  52%|█████▏    | 233M/445M [00:04<00:03, 59.8MB/s]\n",
      "Downloading:  54%|█████▍    | 239M/445M [00:04<00:03, 59.7MB/s]\n",
      "Downloading:  55%|█████▌    | 246M/445M [00:04<00:03, 59.7MB/s]\n",
      "Downloading:  57%|█████▋    | 252M/445M [00:04<00:03, 60.2MB/s]\n",
      "Downloading:  58%|█████▊    | 258M/445M [00:04<00:03, 57.8MB/s]\n",
      "Downloading:  59%|█████▉    | 264M/445M [00:04<00:03, 59.4MB/s]\n",
      "Downloading:  61%|██████    | 270M/445M [00:04<00:02, 59.3MB/s]\n",
      "Downloading:  62%|██████▏   | 276M/445M [00:05<00:02, 59.1MB/s]\n",
      "Downloading:  63%|██████▎   | 282M/445M [00:05<00:02, 56.1MB/s]\n",
      "Downloading:  65%|██████▍   | 289M/445M [00:05<00:02, 59.4MB/s]\n",
      "Downloading:  66%|██████▋   | 295M/445M [00:05<00:02, 59.1MB/s]\n",
      "Downloading:  68%|██████▊   | 301M/445M [00:05<00:02, 58.9MB/s]\n",
      "Downloading:  69%|██████▉   | 307M/445M [00:05<00:02, 59.9MB/s]\n",
      "Downloading:  70%|███████   | 314M/445M [00:05<00:02, 60.6MB/s]\n",
      "Downloading:  72%|███████▏  | 320M/445M [00:05<00:02, 59.5MB/s]\n",
      "Downloading:  73%|███████▎  | 326M/445M [00:05<00:01, 59.7MB/s]\n",
      "Downloading:  75%|███████▍  | 332M/445M [00:05<00:01, 60.2MB/s]\n",
      "Downloading:  76%|███████▌  | 338M/445M [00:06<00:01, 59.7MB/s]\n",
      "Downloading:  77%|███████▋  | 344M/445M [00:06<00:01, 60.0MB/s]\n",
      "Downloading:  79%|███████▊  | 350M/445M [00:06<00:01, 57.8MB/s]\n",
      "Downloading:  80%|████████  | 357M/445M [00:06<00:01, 60.8MB/s]\n",
      "Downloading:  82%|████████▏ | 363M/445M [00:06<00:01, 60.2MB/s]\n",
      "Downloading:  83%|████████▎ | 369M/445M [00:06<00:01, 60.5MB/s]\n",
      "Downloading:  84%|████████▍ | 375M/445M [00:06<00:01, 58.5MB/s]\n",
      "Downloading:  86%|████████▌ | 382M/445M [00:06<00:01, 59.5MB/s]\n",
      "Downloading:  87%|████████▋ | 388M/445M [00:06<00:00, 58.3MB/s]\n",
      "Downloading:  88%|████████▊ | 394M/445M [00:07<00:00, 59.3MB/s]\n",
      "Downloading:  90%|████████▉ | 400M/445M [00:07<00:00, 57.7MB/s]\n",
      "Downloading:  91%|█████████ | 406M/445M [00:07<00:00, 56.7MB/s]\n",
      "Downloading:  93%|█████████▎| 412M/445M [00:07<00:00, 57.9MB/s]\n",
      "Downloading:  94%|█████████▍| 418M/445M [00:07<00:00, 57.1MB/s]\n",
      "Downloading:  95%|█████████▌| 424M/445M [00:07<00:00, 57.0MB/s]\n",
      "Downloading:  97%|█████████▋| 430M/445M [00:07<00:00, 59.5MB/s]\n",
      "Downloading:  98%|█████████▊| 436M/445M [00:07<00:00, 58.6MB/s]\n",
      "Downloading:  99%|█████████▉| 442M/445M [00:07<00:00, 58.1MB/s]\n",
      "Downloading: 100%|██████████| 445M/445M [00:07<00:00, 56.2MB/s]\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2022-11-05 13:05:11,404 INFO] Summarizer(\n",
      "  (bert): Bert(\n",
      "    (model): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Classifier(\n",
      "    (linear1): Linear(in_features=768, out_features=1, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "[2022-11-05 13:05:11,411 INFO] * number of parameters: 110618113\n",
      "[2022-11-05 13:05:11,411 INFO] Start training...\n",
      "Traceback (most recent call last):\n",
      "  File \"SRC/train.py\", line 389, in <module>\n",
      "    train(args, device_id)\n",
      "  File \"SRC/train.py\", line 320, in train\n",
      "    trainer.train(train_iter_fct, args.train_steps)\n",
      "  File \"d:\\ideaproject\\nlp\\text_summarize\\SRC\\models\\trainer.py\", line 133, in train\n",
      "    train_iter = train_iter_fct()\n",
      "  File \"SRC/train.py\", line 301, in train_iter_fct\n",
      "    return data_loader.Dataloader(args, load_dataset(args, 'train', shuffle=True), args.batch_size, device,\n",
      "  File \"d:\\ideaproject\\nlp\\text_summarize\\SRC\\models\\data_loader.py\", line 124, in __init__\n",
      "    self.cur_iter = self._next_dataset_iterator(datasets)\n",
      "  File \"d:\\ideaproject\\nlp\\text_summarize\\SRC\\models\\data_loader.py\", line 145, in _next_dataset_iterator\n",
      "    self.cur_dataset = next(dataset_iter)\n",
      "  File \"d:\\ideaproject\\nlp\\text_summarize\\SRC\\models\\data_loader.py\", line 99, in load_dataset\n",
      "    yield _lazy_dataset_loader(pt, corpus_type)\n",
      "  File \"d:\\ideaproject\\nlp\\text_summarize\\SRC\\models\\data_loader.py\", line 83, in _lazy_dataset_loader\n",
      "    dataset = torch.load(pt_file)\n",
      "  File \"c:\\Users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\serialization.py\", line 771, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"c:\\Users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\serialization.py\", line 270, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"c:\\Users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\serialization.py\", line 251, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'D:\\\\ideaproject/nlp/text_summarize/bert_data/cnndm.train.pt'\n"
     ]
    }
   ],
   "source": [
    "!python SRC/train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b554b0f94921df9660f1661a9fb26d353a9783007f6e2314219fe0a1a83eaf71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
