{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['formal', 'informal', 'android', 'azae', 'chat', 'choding', 'emoticon', 'enfp', 'gentle', 'halbae', 'halmae', 'joongding', 'king', 'naruto', 'seonbi', 'sosim', 'translator']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "data_path = './DATA/korean_smile_style_dataset/smilestyle_dataset.tsv'\n",
    "f= open(data_path,'r',encoding='utf-8')\n",
    "rdr = csv.reader(f, delimiter='\\t' )\n",
    "\n",
    "for line in rdr:\n",
    "    print(line)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(session):\n",
    "    final_data = []\n",
    "    split_session = []\n",
    "    for line in session:\n",
    "        split_session.append(line)\n",
    "        final_data.append(split_session)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data_path = './DATA/korean_smile_style_dataset/smilestyle_dataset.tsv'\n",
    "f= open(data_path,'r',encoding='utf-8')\n",
    "rdr = csv.reader(f, delimiter='\\t')\n",
    "\n",
    "session_dataset = []\n",
    "session = []\n",
    "\n",
    "\n",
    "for i, line in enumerate(rdr):\n",
    "    if i == 0:\n",
    "        header = line\n",
    "    else:\n",
    "        utt = line[0] # format\n",
    "        if utt.strip() != '':\n",
    "            session.append(utt)\n",
    "        else:\n",
    "            session_dataset.append(session)\n",
    "            session = []\n",
    "'''마지막 세션 저장'''\n",
    "session_dataset.append(session)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요. 저는 고양이 6마리 키워요.',\n",
       " '고양이를 6마리나요? 키우는거 안 힘드세요?',\n",
       " '제가 워낙 고양이를 좋아해서 크게 힘들진 않아요.',\n",
       " '가장 나이가 많은 고양이가 어떻게 돼요?',\n",
       " '여섯 살입니다. 갈색 고양이에요.',\n",
       " '그럼 가장 어린 고양이가 어떻게 돼요?',\n",
       " '한 살입니다. 작년에 분양 받았어요.',\n",
       " '그럼 고양이들끼리 안 싸우나요?',\n",
       " '저희 일곱은 다같이 한 가족입니다. 싸우는 일은 없어요.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요, 잘 지내고 있으세요?',\n",
       " '사실 약간 우울한 것 같아요. 그쪽은 잘 지내고 있으세요?',\n",
       " '어째서 우울하세요?',\n",
       " '제가 제일 좋아하는 티비쇼가 꽤 오랫동안 안 나올 것 같아서 슬프네요.',\n",
       " '게임 오브 쓰론 말씀하세요?',\n",
       " '네, 2019년에서야 다시 찍는다고 발표했어요.',\n",
       " '그래도 아예 멈춘 것보다는 나은 것 같아요.',\n",
       " '네, 이야기를 매듭지 않고 끝냈으면 정말 슬펐을 것 같아요.',\n",
       " '그럼 이제는 어떤 쇼를 볼 건가요?',\n",
       " '블랙 미러도 재미있다고 들었는데, 어떤가요?',\n",
       " '저는 티비를 자주 안 봐서 사실 잘 몰라요.',\n",
       " '그러면 직접 알아봐야겠네요.',\n",
       " '새로운 티비쇼를 개척하는 것도 재밌을거에요.',\n",
       " '저도 기대하고 있습니다.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_dataset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eos와 sep토큰이 달라야 논문데로 구현'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(tokenizer)\n",
    "tokenizer.special_tokens_map\n",
    "'''eos와 sep토큰이 달라야 논문데로 구현'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[SEP]', '[SEP]', 2, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.sep_token, tokenizer.eos_token_id, tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'모델이 학습한 데이터구조가 달라졌으므로 적용'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = {'sep_token':'<SEP>'}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "'''모델이 학습한 데이터구조가 달라졌으므로 적용'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[SEP]', '<SEP>', 2, 32000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.sep_token, tokenizer.eos_token_id, tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[MASK]', 4)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token, tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLM 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''MLM (masked language model )마스크 된 부분을 예측하는 것'''\n",
    "import random\n",
    "\n",
    "session = session_dataset[0]\n",
    "mask_ratio = 0.15\n",
    "corrupt_tokens = []\n",
    "output_tokens = []\n",
    "for i, utt in enumerate(session):\n",
    "    original_token = tokenizer.encode(utt, add_special_tokens=False)\n",
    "\n",
    "    mask_num = int(len(original_token)*mask_ratio)\n",
    "    mask_positions = random.sample([x for x in range(len(original_token))], mask_num)\n",
    "    corrupt_token = []\n",
    "    for pos in range(len(original_token)):\n",
    "        if pos in mask_positions:\n",
    "            corrupt_token.append(tokenizer.mask_token_id)\n",
    "        else:\n",
    "            corrupt_token.append(original_token[pos])\n",
    "    \n",
    "    if i == len(session)-1:\n",
    "        output_tokens += original_token\n",
    "        corrupt_tokens += corrupt_token\n",
    "    else:\n",
    "        output_tokens += original_token + [tokenizer.sep_token_id]\n",
    "        corrupt_tokens += corrupt_token + [tokenizer.sep_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 가장 나이가 많은 고양이가 어떻게 돼요? <SEP> 여섯 살입니다. 갈색 고양이에요. <SEP> 그럼 가장 어린 고양이가 어떻게 돼요? <SEP> 한 살입니다. 작년에 분양 받았어요. <SEP> 그럼 고양이들끼리 안 싸우나요? <SEP> 저희 일곱은 다같이 한 가족입니다. 싸우는 일은 없어요.\n",
      "안녕하세요. 저는 고양이 6마리 [MASK]요. <SEP> [MASK]를 6마리나 [MASK]? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 [MASK]게 [MASK]진 않아요. <SEP> 가장 나이가 [MASK]은 고양이가 어떻게 돼요? <SEP> 여섯 살입니다. 갈색 고양이에요 [MASK] <SEP> 그럼 가장 어린 [MASK]가 어떻게 돼요? <SEP> 한 살 [MASK]. 작년에 분양 받았어요. <SEP> 그럼 [MASK]들끼리 안 싸우나요? <SEP> 저희 일곱은 [MASK]같이 한 가족입니다 [MASK] 싸우는 일은 없어요.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output_tokens))\n",
    "print(tokenizer.decode(corrupt_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URC를 위한 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요. 저는 고양이 6마리 키워요.', '고양이를 6마리나요? 키우는거 안 힘드세요?', '제가 워낙 고양이를 좋아해서 크게 힘들진 않아요.', '가장 나이가 많은 고양이가 어떻게 돼요?']\n"
     ]
    }
   ],
   "source": [
    "'''short session'''\n",
    "k = 4 # 논문에서 3개는 context 1개는 response가 가장 성능 좋음\n",
    "short_session_dataset = []\n",
    "for session in session_dataset:\n",
    "    for i in range(len(session)-k+1):\n",
    "        short_session_dataset.append(session[i:i+k])\n",
    "\n",
    "print(short_session_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3430 안녕하세요, 지금까지 저녁은 잘 즐기고 계신가요?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "all_utts = set()\n",
    "for session in session_dataset:\n",
    "    for utt in session:\n",
    "        all_utts.add(utt)\n",
    "\n",
    "all_utts = list(all_utts)\n",
    "print(len(all_utts), all_utts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "session = short_session_dataset[0]\n",
    "urc_tokens = []\n",
    "context_utts = []\n",
    "for i in range(len(session)):\n",
    "    utt = session[i]\n",
    "    original_token = tokenizer.encode(utt, add_special_tokens=False)\n",
    "\n",
    "    if i == len(session)-1:\n",
    "        positive_token = urc_tokens + original_token\n",
    "        while True:\n",
    "            random_neg_response = random.choice(all_utts)\n",
    "            if random_neg_response not in context_utts:\n",
    "                break\n",
    "        random_neg_response_token = tokenizer.encode(random_neg_response, add_special_tokens=False)\n",
    "        random_token = urc_tokens + random_neg_response_token\n",
    "        ''' context nag rep 입력'''\n",
    "        context_neg_response = random.choice(context_utts)\n",
    "        context_neg_response_token = tokenizer.encode(context_neg_response, add_special_tokens=False)\n",
    "        context_neg_tokens = urc_tokens + context_neg_response_token\n",
    "    else:\n",
    "        urc_tokens += original_token + [tokenizer.sep_token_id]\n",
    "    context_utts.append(utt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 가장 나이가 많은 고양이가 어떻게 돼요?\n",
      "#######\n",
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 대단하시네요. 체스 동아리에는 사람이 많이 있나요?\n",
      "#######\n",
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 안녕하세요. 저는 고양이 6마리 키워요.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(positive_token))\n",
    "print('#######')\n",
    "print(tokenizer.decode(random_token))\n",
    "print('#######')\n",
    "print(tokenizer.decode(context_neg_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "post train 데이터 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class post_loader(Dataset):\n",
    "    def __init__(self, data_path) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "        special_tokens = {'sep_token':'<SEP>'}\n",
    "        self.tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "        f = open(data_path, 'r', encoding='utf-8')\n",
    "        rdr = csv.reader(f, delimiter='\\t')\n",
    "\n",
    "        session_dataset = []\n",
    "        session = []\n",
    "\n",
    "\n",
    "        for i, line in enumerate(rdr):\n",
    "            if i == 0:\n",
    "                header = line\n",
    "            else:\n",
    "                utt = line[0] # format\n",
    "                if utt.strip() != '':\n",
    "                    session.append(utt)\n",
    "                else:\n",
    "                    session_dataset.append(session)\n",
    "                    session = []\n",
    "        '''마지막 세션 저장'''\n",
    "        session_dataset.append(session)\n",
    "        f.close()\n",
    "\n",
    "        '''short session context'''\n",
    "        k = 4 # 논문에서 3개는 context 1개는 response가 가장 성능 좋음\n",
    "        self.short_session_dataset = []\n",
    "        for session in session_dataset:\n",
    "            for i in range(len(session)-k+1):\n",
    "                self.short_session_dataset.append(session[i:i+k])\n",
    "        \n",
    "        '''모든 발화 저장'''\n",
    "        self.all_utts = set()\n",
    "        for session in session_dataset:\n",
    "            for utt in session:\n",
    "                self.all_utts.add(utt)\n",
    "\n",
    "        self.all_utts = list(self.all_utts)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.short_session_dataset)\n",
    "\n",
    "    def __getitem__(self, idx) :\n",
    "        session = self.short_session_dataset[idx]\n",
    "        '''MLM'''\n",
    "        mask_ratio = 0.15\n",
    "        self.corrupt_tokens = []\n",
    "        self.output_tokens = []\n",
    "        for i, utt in enumerate(session):\n",
    "            original_token = tokenizer.encode(utt, add_special_tokens=False)\n",
    "\n",
    "            mask_num = int(len(original_token)*mask_ratio)\n",
    "            mask_positions = random.sample([x for x in range(len(original_token))], mask_num)\n",
    "            corrupt_token = []\n",
    "            for pos in range(len(original_token)):\n",
    "                if pos in mask_positions:\n",
    "                    corrupt_token.append(tokenizer.mask_token_id)\n",
    "                else:\n",
    "                    corrupt_token.append(original_token[pos])\n",
    "            \n",
    "            if i == len(session)-1:\n",
    "                self.output_tokens += original_token\n",
    "                self.corrupt_tokens += corrupt_token\n",
    "            else:\n",
    "                self.output_tokens += original_token + [tokenizer.sep_token_id]\n",
    "                self.corrupt_tokens += corrupt_token + [tokenizer.sep_token_id]\n",
    "        \n",
    "        '''label for loss'''\n",
    "        self.corrupt_mask_positions = []\n",
    "        for pos in range(len(self.corrupt_tokens)):\n",
    "            if self.corrupt_tokens[pos] == self.tokenizer.mask_token_id:\n",
    "                self.corrupt_mask_positions.append(pos)\n",
    "        \n",
    "        '''URC'''\n",
    "        urc_tokens = []\n",
    "        context_utts = []\n",
    "        for i in range(len(session)):\n",
    "            utt = session[i]\n",
    "            original_token = self.tokenizer.encode(utt, add_special_tokens=False)\n",
    "\n",
    "            if i == len(session)-1:\n",
    "                urc_tokens += [self.tokenizer.eos_token_id]\n",
    "                self.positive_token = [self.tokenizer.cls_token_id] + urc_tokens + original_token\n",
    "                while True:\n",
    "                    random_neg_response = random.choice(all_utts)\n",
    "                    if random_neg_response not in context_utts:\n",
    "                        break\n",
    "                random_neg_response_token = self.tokenizer.encode(random_neg_response, add_special_tokens=False)\n",
    "                self.random_token = [self.tokenizer.cls_token_id] + urc_tokens + random_neg_response_token\n",
    "                ''' context nag rep 입력'''\n",
    "                context_neg_response = random.choice(context_utts)\n",
    "                context_neg_response_token = self.tokenizer.encode(context_neg_response, add_special_tokens=False)\n",
    "                self.context_neg_tokens = [self.tokenizer.cls_token_id] + urc_tokens + context_neg_response_token\n",
    "            else:\n",
    "                urc_tokens += original_token + [self.tokenizer.sep_token_id]\n",
    "            context_utts.append(utt)\n",
    "        return self.corrupt_tokens, self.output_tokens, self.corrupt_mask_positions, [self.positive_token, self.random_token, self.context_neg_tokens], [0,1,2]\n",
    "\n",
    "    '''배치 처리'''\n",
    "    def collate_fn(self, sessions):\n",
    "        batch_corrupt_tokens, batch_output_tokens, batch_corrupt_mask_positions, batch_urc_inputs, batch_urc_labels = [], [], [], [], []\n",
    "        batch_min_attentions, batch_urc_attentions = [], []\n",
    "        corrupt_max_len, urc_max_len = 0, 0\n",
    "\n",
    "        '''MLM, URC에서 가장 긴 입력 길이 찾기'''\n",
    "        for session in sessions:\n",
    "            corrupt_tokens, output_tokens, corrupt_mask_positions, urc_inputs, urc_labels = session\n",
    "            if len(corrupt_tokens) > corrupt_max_len:\n",
    "                corrupt_max_len = len(corrupt_tokens)\n",
    "            positive_tokens, random_tokens, context_neg_tokens = urc_inputs\n",
    "            if max([len(positive_tokens), len(random_tokens), len(context_neg_tokens)]) > urc_max_len:\n",
    "                urc_max_len = max([len(positive_tokens), len(random_tokens), len(context_neg_tokens)])\n",
    "        \n",
    "        '''padding'''\n",
    "        for session in sessions:\n",
    "            corrupt_tokens, output_tokens, corrupt_mask_positions, urc_inputs, urc_labels = session\n",
    "            '''min'''\n",
    "            batch_corrupt_tokens.append(corrupt_tokens + [self.tokenizer.pad_token_id for _ in range(corrupt_max_len - len(corrupt_tokens))])\n",
    "            batch_min_attentions.append([1 for _ in range(len(corrupt_tokens))] + [0 for _ in range(corrupt_max_len - len(corrupt_tokens))])\n",
    "\n",
    "            batch_output_tokens.append(output_tokens + [self.tokenizer.pad_token_id for _ in range(corrupt_max_len - len(corrupt_tokens))])\n",
    "\n",
    "            batch_corrupt_mask_positions.append(corrupt_mask_positions)\n",
    "            '''urc'''\n",
    "            for urc_input in urc_inputs:\n",
    "                batch_urc_inputs.append(urc_input + [self.tokenizer.pad_token_id for _ in range(urc_max_len - len(urc_input))])\n",
    "                batch_urc_attentions.append([1 for _ in range(len(urc_input))] + [0 for _ in range(urc_max_len - len(urc_input))])\n",
    "\n",
    "            batch_urc_labels += urc_labels\n",
    "        \n",
    "        return torch.tensor(batch_corrupt_tokens), torch.tensor(batch_output_tokens), batch_corrupt_mask_positions, torch.tensor(batch_urc_inputs), \\\n",
    "            torch.tensor(batch_urc_labels), torch.tensor(batch_min_attentions), torch.tensor(batch_urc_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './DATA/korean_smile_style_dataset/smilestyle_dataset.tsv'\n",
    "post_dataset = post_loader(data_path)\n",
    "post_dataloader = DataLoader(post_dataset, batch_size=2, shuffle=False, collate_fn=post_dataset.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_tokens, output_tokens, corrupt_mask_positions, urc_input, urc_label = post_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. [MASK]는 고양이 6마리 키워요. <SEP> [MASK]를 6 [MASK]나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않 [MASK]요 [MASK] <SEP> 가장 나이가 많은 고양이가 [MASK] 돼요?\n",
      "#######\n",
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 가장 나이가 많은 고양이가 어떻게 돼요?\n",
      "#######\n",
      "[4, 13, 16, 39, 41, 50]\n",
      "----------------------------\n",
      "[CLS] 안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> [SEP] 가장 나이가 많은 고양이가 어떻게 돼요?\n",
      "#######\n",
      "[CLS] 안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> [SEP] 안녕하세요. 추석을 잘 보내셨나요?\n",
      "#######\n",
      "[CLS] 안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> [SEP] 고양이를 6마리나요? 키우는거 안 힘드세요?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(corrupt_tokens))\n",
    "print('#######')\n",
    "print(tokenizer.decode(output_tokens))\n",
    "print('#######')\n",
    "print(corrupt_mask_positions)\n",
    "print('----------------------------')\n",
    "print(tokenizer.decode(urc_input[0]))\n",
    "print('#######')\n",
    "print(tokenizer.decode(urc_input[1]))\n",
    "print('#######')\n",
    "print(tokenizer.decode(urc_input[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 5891,  2205,     4,    18,  1535,  2259,  7003,    26, 12736,  5972,\n",
      "          2182,    18, 32000,  7003,     4,    26, 12736,  2075,  2182,    35,\n",
      "             4,  2259,  2180,  1378, 18941,  5971,    35, 32000,  1545,  2116,\n",
      "          7020,     4,  2138, 30581,  1750,  2318,     4,  2043,  1380,  2227,\n",
      "          2182,    18, 32000,  3676,  4358,  2116,  1039,  2073,  7003,  2116,\n",
      "          3842,     4,    35],\n",
      "        [ 7003,  2138,     4, 12736,  2075,  2182,    35,  5688,     4,  2180,\n",
      "          1378, 18941,  5971,    35, 32000,  1545,  2116,  7020,  7003,  2138,\n",
      "             4,     4,  2318,  4390,  2043,  1380,  2227,  2182,    18, 32000,\n",
      "          3676,  4358,  2116,  1039,     4,  7003,  2116,  3842,  6961,    35,\n",
      "         32000,  7070,  1236, 12190,    18, 14008,  7003,     4,  2182,    18,\n",
      "             1,     1,     1]]), tensor([[ 5891,  2205,  5971,    18,  1535,  2259,  7003,    26, 12736,  5972,\n",
      "          2182,    18, 32000,  7003,  2138,    26, 12736,  2075,  2182,    35,\n",
      "          5688,  2259,  2180,  1378, 18941,  5971,    35, 32000,  1545,  2116,\n",
      "          7020,  7003,  2138, 30581,  1750,  2318,  4390,  2043,  1380,  2227,\n",
      "          2182,    18, 32000,  3676,  4358,  2116,  1039,  2073,  7003,  2116,\n",
      "          3842,  6961,    35],\n",
      "        [ 7003,  2138,    26, 12736,  2075,  2182,    35,  5688,  2259,  2180,\n",
      "          1378, 18941,  5971,    35, 32000,  1545,  2116,  7020,  7003,  2138,\n",
      "         30581,  1750,  2318,  4390,  2043,  1380,  2227,  2182,    18, 32000,\n",
      "          3676,  4358,  2116,  1039,  2073,  7003,  2116,  3842,  6961,    35,\n",
      "         32000,  7070,  1236, 12190,    18, 14008,  7003,  2170,  2182,    18,\n",
      "             1,     1,     1]]), [[2, 14, 20, 31, 36, 51], [2, 8, 20, 21, 34, 47]], tensor([[    0,  5891,  2205,  5971,    18,  1535,  2259,  7003,    26, 12736,\n",
      "          5972,  2182,    18, 32000,  7003,  2138,    26, 12736,  2075,  2182,\n",
      "            35,  5688,  2259,  2180,  1378, 18941,  5971,    35, 32000,  1545,\n",
      "          2116,  7020,  7003,  2138, 30581,  1750,  2318,  4390,  2043,  1380,\n",
      "          2227,  2182,    18, 32000,     2,  3676,  4358,  2116,  1039,  2073,\n",
      "          7003,  2116,  3842,  6961,    35,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    0,  5891,  2205,  5971,    18,  1535,  2259,  7003,    26, 12736,\n",
      "          5972,  2182,    18, 32000,  7003,  2138,    26, 12736,  2075,  2182,\n",
      "            35,  5688,  2259,  2180,  1378, 18941,  5971,    35, 32000,  1545,\n",
      "          2116,  7020,  7003,  2138, 30581,  1750,  2318,  4390,  2043,  1380,\n",
      "          2227,  2182,    18, 32000,     2,  3724,  2170,  4230,  5187, 11800,\n",
      "            18,  1535,  2119,   900,  7285,  2379,  4887,  2205,  2259,   575,\n",
      "          2069, 26746,  2182,    18,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    0,  5891,  2205,  5971,    18,  1535,  2259,  7003,    26, 12736,\n",
      "          5972,  2182,    18, 32000,  7003,  2138,    26, 12736,  2075,  2182,\n",
      "            35,  5688,  2259,  2180,  1378, 18941,  5971,    35, 32000,  1545,\n",
      "          2116,  7020,  7003,  2138, 30581,  1750,  2318,  4390,  2043,  1380,\n",
      "          2227,  2182,    18, 32000,     2,  1545,  2116,  7020,  7003,  2138,\n",
      "         30581,  1750,  2318,  4390,  2043,  1380,  2227,  2182,    18,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    0,  7003,  2138,    26, 12736,  2075,  2182,    35,  5688,  2259,\n",
      "          2180,  1378, 18941,  5971,    35, 32000,  1545,  2116,  7020,  7003,\n",
      "          2138, 30581,  1750,  2318,  4390,  2043,  1380,  2227,  2182,    18,\n",
      "         32000,  3676,  4358,  2116,  1039,  2073,  7003,  2116,  3842,  6961,\n",
      "            35, 32000,     2,  7070,  1236, 12190,    18, 14008,  7003,  2170,\n",
      "          2182,    18,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    0,  7003,  2138,    26, 12736,  2075,  2182,    35,  5688,  2259,\n",
      "          2180,  1378, 18941,  5971,    35, 32000,  1545,  2116,  7020,  7003,\n",
      "          2138, 30581,  1750,  2318,  4390,  2043,  1380,  2227,  2182,    18,\n",
      "         32000,  3676,  4358,  2116,  1039,  2073,  7003,  2116,  3842,  6961,\n",
      "            35, 32000,     2,  5378,  2170,  2259,  3723,  7124,  2628,  2170,\n",
      "          1513,  2359, 13964,    16, 18052,  2138, 17397,  3719,  2200,  2259,\n",
      "          3931,  2170, 24135,  2116,  2088,  5602, 27135,  2154,  7124,  2205,\n",
      "          2259,  8839,  3606,    18],\n",
      "        [    0,  7003,  2138,    26, 12736,  2075,  2182,    35,  5688,  2259,\n",
      "          2180,  1378, 18941,  5971,    35, 32000,  1545,  2116,  7020,  7003,\n",
      "          2138, 30581,  1750,  2318,  4390,  2043,  1380,  2227,  2182,    18,\n",
      "         32000,  3676,  4358,  2116,  1039,  2073,  7003,  2116,  3842,  6961,\n",
      "            35, 32000,     2,  1545,  2116,  7020,  7003,  2138, 30581,  1750,\n",
      "          2318,  4390,  2043,  1380,  2227,  2182,    18,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]]), tensor([0, 1, 2, 0, 1, 2]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "for data in post_dataloader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained('klue/roberta-base')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.eval()\n",
    "# MLM은 mask 토큰을 재구성 하는 것이기 때문에 \n",
    "# 출력 size가 사전 크기여야 하는데\n",
    "# automodel로 가져오면 출력 크기가 다르다\n",
    "# dense layer에서 감소하기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 768)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaForMaskedLM, AutoTokenizer\n",
    "model = RobertaForMaskedLM.from_pretrained('klue/roberta-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "\n",
    "special_tokens = {'sep_token':'<SEP>'}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PostModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(PostModel, self).__init__()\n",
    "        self.model = RobertaForMaskedLM.from_pretrained('klue/roberta-base')\n",
    "        self.hiddenDim = self.model.config.hidden_size\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "        special_tokens = {'sep_token':'<SEP>'}\n",
    "        self.tokenizer.add_special_tokens(special_tokens)\n",
    "        self.model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        self.W = nn.Linear(self.hiddenDim, 3)\n",
    "\n",
    "    def forword(self, batch_corrupt_tokens, batch_corrupt_mask_positions, batch_urc_inputs, batch_min_attentions, batch_urc_attentions):\n",
    "        # MLM\n",
    "        corrupt_outputs = self.model(batch_corrupt_tokens, attention_mask = batch_min_attentions)['logit'] # [2, 53, 32001]\n",
    "        corrupt_mask_outputs = []\n",
    "\n",
    "        for i_batch in range(len(batch_corrupt_mask_positions)):\n",
    "            corrupt_mask_output = []\n",
    "            batch_corrupt_mask_position = batch_corrupt_mask_positions[i_batch]\n",
    "            for pos in batch_corrupt_mask_position:\n",
    "                corrupt_mask_output.append(corrupt_outputs[i_batch, pos, :].unsqueeze(0)) # [1, 32001]\n",
    "            corrupt_mask_outputs.append(torch.cat(corrupt_mask_output, 0)) # [mask_num, 32001]\n",
    "\n",
    "        # URC\n",
    "        urc_outputs = self.model(batch_urc_inputs, attention_mask=batch_urc_attentions, output_hidden_states=True)['hidden_states'][-1] # [6,61,768]\n",
    "        urc_logits = self.W(urc_outputs) # [6,61,3]\n",
    "        urc_cls_outputs = urc_logits[:,0,:] # [6, 3]\n",
    "\n",
    "        return corrupt_mask_outputs, urc_cls_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SRC.model import PostModel\n",
    "post_model = PostModel().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "def CELoss(pred_outs, labels):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_val = loss(pred_outs,labels)\n",
    "    return loss_val\n",
    "\n",
    "def SaveModel(epoch, model, path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    torch.save({'epoch' : epoch, 'model' : model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1381 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Module [PostModel] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m batch_urc_attentions \u001b[39m=\u001b[39m batch_urc_attentions\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     46\u001b[0m \u001b[39m\"\"\"Prediction\"\"\"\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m corrupt_mask_outputs, urc_cls_outputs \u001b[39m=\u001b[39m post_model(batch_corrupt_tokens, batch_corrupt_mask_positions, batch_urc_inputs, batch_mlm_attentions, batch_urc_attentions)\n\u001b[0;32m     48\u001b[0m \u001b[39m\"\"\"Loss calculation & training\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m original_token_indexs \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\answl\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:244\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_unimplemented\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \n\u001b[0;32m    236\u001b[0m \u001b[39m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[39m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModule [\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] is missing the required \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mforward\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m function\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Module [PostModel] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import os\n",
    "\n",
    "def CELoss(pred_outs, labels):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_val = loss(pred_outs,labels)\n",
    "    return loss_val\n",
    "\n",
    "def SaveModel(model_info, path, model_name):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    torch.save(model_info, os.path.join(path, model_name))\n",
    "\n",
    "from SRC.dataset import post_loader\n",
    "from SRC.model import PostModel\n",
    "\n",
    "data_path = './DATA/korean_smile_style_dataset/smilestyle_dataset.tsv'\n",
    "post_model = PostModel().cuda()\n",
    "post_dataset = post_loader(data_path)\n",
    "post_dataloader = DataLoader(post_dataset, batch_size=2, shuffle= False, collate_fn=post_dataset.collate_fn)\n",
    "\n",
    "training_epochs = 1\n",
    "max_grad_norm = 10\n",
    "lr = 1e-5\n",
    "num_traning_steps = len(post_dataset)*training_epochs\n",
    "num_warmup_steps = len(post_dataset)\n",
    "optimizer = torch.optim.AdamW(post_model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_traning_steps, num_training_steps=num_traning_steps)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    post_model.train()\n",
    "    for i_batch, data in enumerate(tqdm (post_dataloader)):\n",
    "        batch_corrupt_tokens, batch_output_tokens, batch_corrupt_mask_positions, batch_urc_inputs, batch_urc_labels, batch_mlm_attentions, batch_urc_attentions = data\n",
    "        batch_corrupt_tokens = batch_corrupt_tokens.cuda()\n",
    "        batch_output_tokens = batch_output_tokens.cuda()\n",
    "        batch_urc_inputs = batch_urc_inputs.cuda()\n",
    "        batch_urc_labels = batch_urc_labels.cuda()\n",
    "        batch_mlm_attentions = batch_mlm_attentions.cuda()\n",
    "        batch_urc_attentions = batch_urc_attentions.cuda()\n",
    "\n",
    "        \"\"\"Prediction\"\"\"\n",
    "        corrupt_mask_outputs, urc_cls_outputs = post_model(batch_corrupt_tokens, batch_corrupt_mask_positions, batch_urc_inputs, batch_mlm_attentions, batch_urc_attentions)\n",
    "        \"\"\"Loss calculation & training\"\"\"\n",
    "        original_token_indexs = []\n",
    "        for i_batch in range (len (batch_corrupt_mask_positions)):\n",
    "            original_token_index = []\n",
    "            batch_corrupt_mask_position = batch_corrupt_mask_positions[i_batch]\n",
    "            for pos in batch_corrupt_mask_position:\n",
    "                original_token_index.append(batch_output_tokens[i_batch, pos].item())\n",
    "            original_token_indexs.append(original_token_index)\n",
    "        mlm_loss = 0\n",
    "        for corrupt_mask_output, original_token_index in zip(corrupt_mask_outputs, original_token_indexs):\n",
    "            mlm_loss += CELoss (corrupt_mask_output, torch.tensor (original_token_index). cuda ( ) )\n",
    "        urc_loss = CELoss (urc_cls_outputs, batch_urc_labels)\n",
    "        loss_val= mlm_loss + urc_loss\n",
    "        loss_val.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(post_model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad ()\n",
    "        model_info = {'epoch' : epoch, 'model' : post_model.state_dict(), 'optimizer' : optimizer, 'loss' : loss_val}\n",
    "SaveModel(model_info, path='./MODEL/', model_name=f'PostModel_{epoch}.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe8c363594e410b861697262b3b93128925b49060e3d4f22e3f8363715822225"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
